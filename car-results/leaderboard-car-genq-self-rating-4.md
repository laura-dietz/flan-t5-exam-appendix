CAR, Generated Questions, Self-ratings (>= 4), ExamCover

| method | exam | +/- | exam-std | n-exam | +/- | n-exam-std | orig\_TREC\_leaderboard\_rank | orig\_EXAM\_leaderboard\_rank |  |  |  |  |  |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | --- | --- | --- |
| Bert-ConvKNRM | 0.503 | +/- | 0.011 | 0.615 | +/- | 0.011 |  |  |  |  |  |  |  |
| Bert-ConvKNRM-50 | 0.605 | +/- | 0.011 | 0.743 | +/- | 0.009 | 9.000 | 6.000 |  |  |  |  |  |
| Bert-DRMMTKS | 0.366 | +/- | 0.013 | 0.447 | +/- | 0.015 | 12.000 | 3.000 |  |  |  |  |  |
| ECNU\_BM25 | 0.604 | +/- | 0.012 | 0.741 | +/- | 0.010 |  |  |  |  |  |  |  |
| ECNU\_BM25\_1 | 0.602 | +/- | 0.011 | 0.738 | +/- | 0.010 | 7.500 | 11.000 |  |  |  |  |  |
| ECNU\_ReRank1 | 0.572 | +/- | 0.011 | 0.700 | +/- | 0.009 | 7.500 | 12.000 |  |  |  |  |  |
| ICT-BM25 | 0.598 | +/- | 0.011 | 0.734 | +/- | 0.010 |  |  |  |  |  |  |  |
| ICT-DRMMTKS | 0.369 | +/- | 0.014 | 0.449 | +/- | 0.015 | 16.000 | 13.000 |  |  |  |  |  |
| IRIT1 | 0.569 | +/- | 0.012 | 0.698 | +/- | 0.011 | 5.000 | 7.000 |  |  |  |  |  |
| IRIT2 | 0.569 | +/- | 0.012 | 0.698 | +/- | 0.011 | 5.000 | 4.000 |  |  |  |  |  |
| IRIT3 | 0.569 | +/- | 0.012 | 0.698 | +/- | 0.011 | 5.000 | 9.000 |  |  |  |  |  |
| ReRnak2\_BERT | 0.623 | +/- | 0.010 | 0.765 | +/- | 0.009 | 3.000 | 1.000 |  |  |  |  |  |
| ReRnak3\_BERT | 0.62 | +/- | 0.010 | 0.760 | +/- | 0.008 | 2.000 | 5.000 |  |  |  |  |  |
| UNH-bm25-ecmpsg | 0.522 | +/- | 0.013 | 0.640 | +/- | 0.013 | 11.000 | 10.000 |  |  |  |  |  |
| UNH-bm25-rm | 0.545 | +/- | 0.012 | 0.668 | +/- | 0.012 |  |  |  |  |  |  |  |
| UNH-bm25-stem | 0.522 | +/- | 0.013 | 0.640 | +/- | 0.013 |  |  |  |  |  |  |  |
| UNH-dl100 | 0.522 | +/- | 0.013 | 0.640 | +/- | 0.013 |  |  |  |  |  |  |  |
| UNH-dl300 | 0.522 | +/- | 0.013 | 0.640 | +/- | 0.013 |  |  |  |  |  |  |  |
| UNH-ecn | 0.522 | +/- | 0.012 | 0.639 | +/- | 0.011 |  |  |  |  |  |  |  |
| UNH-qee | 0.614 | +/- | 0.011 | 0.753 | +/- | 0.010 |  |  |  |  |  |  |  |
| UNH-tfidf-lem | 0.522 | +/- | 0.013 | 0.640 | +/- | 0.013 |  |  |  |  |  |  |  |
| UNH-tfidf-ptsim | 0.522 | +/- | 0.013 | 0.640 | +/- | 0.013 |  |  |  |  |  |  |  |
| UNH-tfidf-stem | 0.522 | +/- | 0.013 | 0.640 | +/- | 0.013 |  |  |  |  |  |  |  |
| UvABM25RM3 | 0.26 | +/- | 0.015 | 0.315 | +/- | 0.017 | 13.000 | 15.000 |  |  |  |  |  |
| UvABottomUp1 | 0.209 | +/- | 0.011 | 0.252 | +/- | 0.013 |  |  |  |  |  |  |  |
| UvABottomUp2 | 0.251 | +/- | 0.014 | 0.306 | +/- | 0.017 | 15.000 | 16.000 |  |  |  |  |  |
| UvABottomUpChangeOrder | 0.255 | +/- | 0.014 | 0.309 | +/- | 0.017 | 14.000 | 14.000 |  |  |  |  |  |
| \_overall\_ | 0.811 | +/- | 0.008 | 1.000 | +/- | 0.000 |  |  |  |  |  |  |  |
| bm25-populated | 0.547 | +/- | 0.012 | 0.670 | +/- | 0.011 | 10.000 | 8.000 |  |  |  |  |  |
| dangnt-nlp | 0.609 | +/- | 0.012 | 0.747 | +/- | 0.011 | 1.000 | 2.000 |  |  |  |  |  |
| spearman | 0.869 |  |  | 0.869 |  |  |  |  |  |  |  |  |  |
| kendall | 0.687 |  |  | 0.687 |  |  |  |  |  |  |  |  |  |
| EXAM scores produced with GradeFilter(model\_name=None, prompt\_class='QuestionSelfRatedUnanswerablePromptWithChoices', is\_self\_rated=None, min\_self\_rating=None, question\_set='genq') |  |  |  |  |  |  |  |  |  |  |  |  |  |
| min\_rating | 4 |  |  |  |  |  |  |  |  |  |  |  |  |

