method	exam	+/-	exam-std	n-exam	+/-	n-exam-std	orig_TREC_leaderboard_rank	orig_EXAM_leaderboard_rank
Bert-ConvKNRM	0.248	+/-	0.014	0.581	+/-	0.027	 	 
Bert-ConvKNRM-50	0.277	+/-	0.014	0.651	+/-	0.026	9.000	6.000
Bert-DRMMTKS	0.247	+/-	0.014	0.583	+/-	0.028	12.000	3.000
ECNU_BM25	0.278	+/-	0.015	0.648	+/-	0.027	 	 
ECNU_BM25_1	0.278	+/-	0.015	0.647	+/-	0.027	7.500	11.000
ECNU_ReRank1	0.285	+/-	0.016	0.660	+/-	0.028	7.500	12.000
ICT-BM25	0.278	+/-	0.015	0.648	+/-	0.026	 	 
ICT-DRMMTKS	0.237	+/-	0.014	0.543	+/-	0.028	16.000	13.000
IRIT1	0.279	+/-	0.015	0.643	+/-	0.028	5.000	7.000
IRIT2	0.279	+/-	0.015	0.643	+/-	0.028	5.000	4.000
IRIT3	0.279	+/-	0.015	0.643	+/-	0.028	5.000	9.000
ReRnak2_BERT	0.281	+/-	0.015	0.645	+/-	0.028	3.000	1.000
ReRnak3_BERT	0.288	+/-	0.015	0.667	+/-	0.027	2.000	5.000
UNH-bm25-ecmpsg	0.267	+/-	0.015	0.623	+/-	0.028	11.000	10.000
UNH-bm25-rm	0.276	+/-	0.014	0.651	+/-	0.027	 	 
UNH-bm25-stem	0.267	+/-	0.015	0.623	+/-	0.028	 	 
UNH-dl100	0.267	+/-	0.015	0.623	+/-	0.028	 	 
UNH-dl300	0.267	+/-	0.015	0.623	+/-	0.028	 	 
UNH-ecn	0.259	+/-	0.014	0.592	+/-	0.027	 	 
UNH-qee	0.271	+/-	0.014	0.634	+/-	0.028	 	 
UNH-tfidf-lem	0.267	+/-	0.015	0.623	+/-	0.028	 	 
UNH-tfidf-ptsim	0.267	+/-	0.015	0.623	+/-	0.028	 	 
UNH-tfidf-stem	0.267	+/-	0.015	0.623	+/-	0.028	 	 
UvABM25RM3	0.147	+/-	0.010	0.358	+/-	0.024	13.000	15.000
UvABottomUp1	0.142	+/-	0.011	0.333	+/-	0.024	 	 
UvABottomUp2	0.148	+/-	0.010	0.350	+/-	0.023	15.000	16.000
UvABottomUpChangeOrder	0.145	+/-	0.010	0.353	+/-	0.024	14.000	14.000
_overall_	0.388	+/-	0.016	1.000	+/-	0.000	 	 
bm25-populated	0.275	+/-	0.015	0.635	+/-	0.027	10.000	8.000
dangnt-nlp	0.300	+/-	0.016	0.679	+/-	0.028	1.000	2.000
spearman	0.937			0.853				
kendall	0.841			0.721				
 EXAM scores produced with GradeFilter(model_name='google/flan-t5-large', prompt_class='QuestionCompleteConcisePromptWithAnswerKey2', is_self_rated=None, min_self_rating=None, question_set='tqa')
 min_rating	None



